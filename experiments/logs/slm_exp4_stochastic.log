Using device: cuda
Config: student=gpt2, ref=gpt2, epochs=1, batch_size=8, select_ratio=0.5, selection=stochastic, seed=42
Loading WikiText-2...
Train blocks: 18666 Val blocks: 1931
Loading reference model: gpt2
Loading student model from: gpt2
Starting SLM training with select_ratio = 0.5
Epoch 1 Step 100/2334 - SLM loss (selected tokens only): 4.3023
Epoch 1 Step 200/2334 - SLM loss (selected tokens only): 4.2038
Epoch 1 Step 300/2334 - SLM loss (selected tokens only): 4.1589
Epoch 1 Step 400/2334 - SLM loss (selected tokens only): 4.0957
Epoch 1 Step 500/2334 - SLM loss (selected tokens only): 4.1024
Epoch 1 Step 600/2334 - SLM loss (selected tokens only): 4.0673
Epoch 1 Step 700/2334 - SLM loss (selected tokens only): 4.0446
Epoch 1 Step 800/2334 - SLM loss (selected tokens only): 4.0444
Epoch 1 Step 900/2334 - SLM loss (selected tokens only): 4.0295
Epoch 1 Step 1000/2334 - SLM loss (selected tokens only): 4.0546
Epoch 1 Step 1100/2334 - SLM loss (selected tokens only): 3.9804
Epoch 1 Step 1200/2334 - SLM loss (selected tokens only): 3.9716
Epoch 1 Step 1300/2334 - SLM loss (selected tokens only): 4.0024
Epoch 1 Step 1400/2334 - SLM loss (selected tokens only): 3.9838
Epoch 1 Step 1500/2334 - SLM loss (selected tokens only): 3.9850
Epoch 1 Step 1600/2334 - SLM loss (selected tokens only): 3.9797
Epoch 1 Step 1700/2334 - SLM loss (selected tokens only): 3.9288
Epoch 1 Step 1800/2334 - SLM loss (selected tokens only): 3.9250
Epoch 1 Step 1900/2334 - SLM loss (selected tokens only): 3.9278
Epoch 1 Step 2000/2334 - SLM loss (selected tokens only): 3.9411
Epoch 1 Step 2100/2334 - SLM loss (selected tokens only): 3.9257
Epoch 1 Step 2200/2334 - SLM loss (selected tokens only): 3.9449
Epoch 1 Step 2300/2334 - SLM loss (selected tokens only): 3.8747
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
End of epoch 1: val_loss (full tokens) = 3.4539, val_ppl = 31.62
SLM training complete.
