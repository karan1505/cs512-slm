Using device: cuda
Config: student=gpt2, ref=gpt2, epochs=1, batch_size=8, select_ratio=0.5, selection=random, seed=42
Loading WikiText-2...
Train blocks: 18666 Val blocks: 1931
Loading reference model: gpt2
Loading student model from: gpt2
Starting SLM training with select_ratio = 0.5
Epoch 1 Step 100/2334 - SLM loss (selected tokens only): 3.8872
Epoch 1 Step 200/2334 - SLM loss (selected tokens only): 3.7446
Epoch 1 Step 300/2334 - SLM loss (selected tokens only): 3.6981
Epoch 1 Step 400/2334 - SLM loss (selected tokens only): 3.6485
Epoch 1 Step 500/2334 - SLM loss (selected tokens only): 3.6168
Epoch 1 Step 600/2334 - SLM loss (selected tokens only): 3.5783
Epoch 1 Step 700/2334 - SLM loss (selected tokens only): 3.5742
Epoch 1 Step 800/2334 - SLM loss (selected tokens only): 3.5468
Epoch 1 Step 900/2334 - SLM loss (selected tokens only): 3.5904
Epoch 1 Step 1000/2334 - SLM loss (selected tokens only): 3.5745
Epoch 1 Step 1100/2334 - SLM loss (selected tokens only): 3.5267
Epoch 1 Step 1200/2334 - SLM loss (selected tokens only): 3.5049
Epoch 1 Step 1300/2334 - SLM loss (selected tokens only): 3.5366
Epoch 1 Step 1400/2334 - SLM loss (selected tokens only): 3.5398
Epoch 1 Step 1500/2334 - SLM loss (selected tokens only): 3.5105
Epoch 1 Step 1600/2334 - SLM loss (selected tokens only): 3.5040
Epoch 1 Step 1700/2334 - SLM loss (selected tokens only): 3.4762
Epoch 1 Step 1800/2334 - SLM loss (selected tokens only): 3.4871
Epoch 1 Step 1900/2334 - SLM loss (selected tokens only): 3.4699
Epoch 1 Step 2000/2334 - SLM loss (selected tokens only): 3.4694
Epoch 1 Step 2100/2334 - SLM loss (selected tokens only): 3.4749
Epoch 1 Step 2200/2334 - SLM loss (selected tokens only): 3.4960
Epoch 1 Step 2300/2334 - SLM loss (selected tokens only): 3.4502
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
End of epoch 1: val_loss (full tokens) = 3.4365, val_ppl = 31.08
SLM training complete.
