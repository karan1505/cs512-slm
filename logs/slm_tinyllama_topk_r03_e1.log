Loading dataset…
Data ready. Steps per epoch: 11578 (batch=4, grad_accum=4)
Loading reference model…
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading student base model…
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Attaching LoRA adapters from baseline_tinyllama_lora_bs4_ga4_e1 …
trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338
Starting SLM-LoRA training… selection=topk, select_ratio=0.3
Epoch 1/1 …
/home/karans/miniconda3/envs/slm512/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/karans/miniconda3/envs/slm512/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")

opt_step 20 | slm_loss (selected) 9.3161 | 28.9s for last 20 opt steps | ~2874 opt steps left
.
opt_step 40 | slm_loss (selected) 9.3922 | 28.5s for last 20 opt steps | ~2854 opt steps left
.
opt_step 60 | slm_loss (selected) 8.8115 | 30.0s for last 20 opt steps | ~2834 opt steps left
.
opt_step 80 | slm_loss (selected) 9.3365 | 29.4s for last 20 opt steps | ~2814 opt steps left
.
opt_step 100 | slm_loss (selected) 9.2042 | 30.3s for last 20 opt steps | ~2794 opt steps left

opt_step 120 | slm_loss (selected) 8.8679 | 35.6s for last 20 opt steps | ~2774 opt steps left
.
opt_step 140 | slm_loss (selected) 8.8844 | 31.1s for last 20 opt steps | ~2754 opt steps left
.
opt_step 160 | slm_loss (selected) 8.9337 | 35.6s for last 20 opt steps | ~2734 opt steps left
.
opt_step 180 | slm_loss (selected) 8.8463 | 33.1s for last 20 opt steps | ~2714 opt steps left
.
opt_step 200 | slm_loss (selected) 8.8802 | 33.3s for last 20 opt steps | ~2694 opt steps left

opt_step 220 | slm_loss (selected) 8.9151 | 34.5s for last 20 opt steps | ~2674 opt steps left
.
opt_step 240 | slm_loss (selected) 8.6624 | 35.6s for last 20 opt steps | ~2654 opt steps left
.
opt_step 260 | slm_loss (selected) 9.0055 | 34.1s for last 20 opt steps | ~2634 opt steps left
.
opt_step 280 | slm_loss (selected) 8.6805 | 34.0s for last 20 opt steps | ~2614 opt steps left
.
opt_step 300 | slm_loss (selected) 8.7771 | 31.2s for last 20 opt steps | ~2594 opt steps left

opt_step 320 | slm_loss (selected) 8.8945 | 45.4s for last 20 opt steps | ~2574 opt steps left
.
opt_step 340 | slm_loss (selected) 8.9150 | 38.4s for last 20 opt steps | ~2554 opt steps left
.
opt_step 360 | slm_loss (selected) 9.1756 | 37.1s for last 20 opt steps | ~2534 opt steps left
.
opt_step 380 | slm_loss (selected) 9.0051 | 38.9s for last 20 opt steps | ~2514 opt steps left
.
opt_step 400 | slm_loss (selected) 8.7524 | 42.2s for last 20 opt steps | ~2494 opt steps left

opt_step 420 | slm_loss (selected) 8.6094 | 30.7s for last 20 opt steps | ~2474 opt steps left
.
opt_step 440 | slm_loss (selected) 8.8491 | 35.3s for last 20 opt steps | ~2454 opt steps left
.
opt_step 460 | slm_loss (selected) 8.7231 | 31.4s for last 20 opt steps | ~2434 opt steps left
.
opt_step 480 | slm_loss (selected) 8.7493 | 33.1s for last 20 opt steps | ~2414 opt steps left
.
opt_step 500 | slm_loss (selected) 8.6848 | 31.0s for last 20 opt steps | ~2394 opt steps left

opt_step 520 | slm_loss (selected) 9.1193 | 30.8s for last 20 opt steps | ~2374 opt steps left
.
opt_step 540 | slm_loss (selected) 9.0053 | 30.7s for last 20 opt steps | ~2354 opt steps left
.
opt_step 560 | slm_loss (selected) 8.9412 | 30.2s for last 20 opt steps | ~2334 opt steps left
.
opt_step 580 | slm_loss (selected) 8.4191 | 30.4s for last 20 opt steps | ~2314 opt steps left
.
opt_step 600 | slm_loss (selected) 8.9547 | 37.2s for last 20 opt steps | ~2294 opt steps left

opt_step 620 | slm_loss (selected) 8.7624 | 43.9s for last 20 opt steps | ~2274 opt steps left
.
opt_step 640 | slm_loss (selected) 8.8165 | 34.4s for last 20 opt steps | ~2254 opt steps left
.
opt_step 660 | slm_loss (selected) 8.5474 | 31.0s for last 20 opt steps | ~2234 opt steps left
.
opt_step 680 | slm_loss (selected) 8.7966 | 31.0s for last 20 opt steps | ~2214 opt steps left
.
opt_step 700 | slm_loss (selected) 8.5082 | 36.3s for last 20 opt steps | ~2194 opt steps left

opt_step 720 | slm_loss (selected) 9.0058 | 45.8s for last 20 opt steps | ~2174 opt steps left
.
opt_step 740 | slm_loss (selected) 8.7646 | 35.0s for last 20 opt steps | ~2154 opt steps left
.
opt_step 760 | slm_loss (selected) 8.8058 | 36.0s for last 20 opt steps | ~2134 opt steps left
.
opt_step 780 | slm_loss (selected) 8.9456 | 40.5s for last 20 opt steps | ~2114 opt steps left
.
opt_step 800 | slm_loss (selected) 9.0105 | 42.5s for last 20 opt steps | ~2094 opt steps left

opt_step 820 | slm_loss (selected) 9.0104 | 48.3s for last 20 opt steps | ~2074 opt steps left
.
opt_step 840 | slm_loss (selected) 8.8087 | 41.7s for last 20 opt steps | ~2054 opt steps left
.
opt_step 860 | slm_loss (selected) 8.9526 | 34.6s for last 20 opt steps | ~2034 opt steps left
.
opt_step 880 | slm_loss (selected) 9.1078 | 40.6s for last 20 opt steps | ~2014 opt steps left
.
opt_step 900 | slm_loss (selected) 8.9883 | 49.0s for last 20 opt steps | ~1994 opt steps left

opt_step 920 | slm_loss (selected) 9.0427 | 40.2s for last 20 opt steps | ~1974 opt steps left
.
opt_step 940 | slm_loss (selected) 8.5637 | 32.7s for last 20 opt steps | ~1954 opt steps left
.
opt_step 960 | slm_loss (selected) 8.8230 | 32.0s for last 20 opt steps | ~1934 opt steps left
.
opt_step 980 | slm_loss (selected) 8.9054 | 32.5s for last 20 opt steps | ~1914 opt steps left
.
opt_step 1000 | slm_loss (selected) 9.1297 | 32.0s for last 20 opt steps | ~1894 opt steps left

opt_step 1020 | slm_loss (selected) 8.8744 | 32.1s for last 20 opt steps | ~1874 opt steps left
.
opt_step 1040 | slm_loss (selected) 9.2309 | 32.1s for last 20 opt steps | ~1854 opt steps left
.
opt_step 1060 | slm_loss (selected) 9.3238 | 32.2s for last 20 opt steps | ~1834 opt steps left
.
opt_step 1080 | slm_loss (selected) 8.6500 | 31.9s for last 20 opt steps | ~1814 opt steps left
.
opt_step 1100 | slm_loss (selected) 9.0732 | 32.1s for last 20 opt steps | ~1794 opt steps left

opt_step 1120 | slm_loss (selected) 8.9311 | 32.3s for last 20 opt steps | ~1774 opt steps left
.
opt_step 1140 | slm_loss (selected) 8.8128 | 32.0s for last 20 opt steps | ~1754 opt steps left
.
opt_step 1160 | slm_loss (selected) 8.9491 | 32.2s for last 20 opt steps | ~1734 opt steps left
.
opt_step 1180 | slm_loss (selected) 9.2941 | 31.9s for last 20 opt steps | ~1714 opt steps left
.
opt_step 1200 | slm_loss (selected) 9.0360 | 32.0s for last 20 opt steps | ~1694 opt steps left

opt_step 1220 | slm_loss (selected) 8.8550 | 32.6s for last 20 opt steps | ~1674 opt steps left
.
opt_step 1240 | slm_loss (selected) 9.1183 | 32.1s for last 20 opt steps | ~1654 opt steps left
.
opt_step 1260 | slm_loss (selected) 8.9795 | 32.0s for last 20 opt steps | ~1634 opt steps left
.
opt_step 1280 | slm_loss (selected) 8.7791 | 32.0s for last 20 opt steps | ~1614 opt steps left
.
opt_step 1300 | slm_loss (selected) 9.2898 | 32.0s for last 20 opt steps | ~1594 opt steps left

opt_step 1320 | slm_loss (selected) 8.9168 | 32.4s for last 20 opt steps | ~1574 opt steps left
.
opt_step 1340 | slm_loss (selected) 9.1515 | 31.9s for last 20 opt steps | ~1554 opt steps left
.
opt_step 1360 | slm_loss (selected) 9.0041 | 31.9s for last 20 opt steps | ~1534 opt steps left
.
opt_step 1380 | slm_loss (selected) 8.8154 | 39.2s for last 20 opt steps | ~1514 opt steps left
.
opt_step 1400 | slm_loss (selected) 9.0032 | 33.2s for last 20 opt steps | ~1494 opt steps left

opt_step 1420 | slm_loss (selected) 9.1135 | 33.8s for last 20 opt steps | ~1474 opt steps left
.
opt_step 1440 | slm_loss (selected) 8.8768 | 34.5s for last 20 opt steps | ~1454 opt steps left
.
opt_step 1460 | slm_loss (selected) 9.3608 | 39.9s for last 20 opt steps | ~1434 opt steps left
.
opt_step 1480 | slm_loss (selected) 8.7886 | 37.7s for last 20 opt steps | ~1414 opt steps left
.
opt_step 1500 | slm_loss (selected) 8.9656 | 36.1s for last 20 opt steps | ~1394 opt steps left

opt_step 1520 | slm_loss (selected) 9.0374 | 70.9s for last 20 opt steps | ~1374 opt steps left
.
opt_step 1540 | slm_loss (selected) 9.1163 | 42.1s for last 20 opt steps | ~1354 opt steps left
.
opt_step 1560 | slm_loss (selected) 9.0749 | 30.8s for last 20 opt steps | ~1334 opt steps left
.
opt_step 1580 | slm_loss (selected) 9.1904 | 30.6s for last 20 opt steps | ~1314 opt steps left
.
opt_step 1600 | slm_loss (selected) 9.4778 | 32.2s for last 20 opt steps | ~1294 opt steps left

opt_step 1620 | slm_loss (selected) 8.7552 | 31.0s for last 20 opt steps | ~1274 opt steps left
.
opt_step 1640 | slm_loss (selected) 8.8554 | 44.7s for last 20 opt steps | ~1254 opt steps left
.
opt_step 1660 | slm_loss (selected) 9.1920 | 47.1s for last 20 opt steps | ~1234 opt steps left
.
opt_step 1680 | slm_loss (selected) 8.8792 | 34.6s for last 20 opt steps | ~1214 opt steps left
.
opt_step 1700 | slm_loss (selected) 9.0960 | 34.4s for last 20 opt steps | ~1194 opt steps left

opt_step 1720 | slm_loss (selected) 9.0713 | 51.8s for last 20 opt steps | ~1174 opt steps left
.
opt_step 1740 | slm_loss (selected) 8.8480 | 30.8s for last 20 opt steps | ~1154 opt steps left
.
opt_step 1760 | slm_loss (selected) 9.2233 | 30.6s for last 20 opt steps | ~1134 opt steps left
.
opt_step 1780 | slm_loss (selected) 9.4350 | 31.0s for last 20 opt steps | ~1114 opt steps left
.
opt_step 1800 | slm_loss (selected) 8.7270 | 30.7s for last 20 opt steps | ~1094 opt steps left

opt_step 1820 | slm_loss (selected) 9.3476 | 30.9s for last 20 opt steps | ~1074 opt steps left
.
opt_step 1840 | slm_loss (selected) 9.5398 | 30.9s for last 20 opt steps | ~1054 opt steps left
.
opt_step 1860 | slm_loss (selected) 8.9035 | 31.0s for last 20 opt steps | ~1034 opt steps left
.
opt_step 1880 | slm_loss (selected) 9.3497 | 30.8s for last 20 opt steps | ~1014 opt steps left
.
opt_step 1900 | slm_loss (selected) 9.1778 | 30.8s for last 20 opt steps | ~994 opt steps left

opt_step 1920 | slm_loss (selected) 8.9805 | 30.7s for last 20 opt steps | ~974 opt steps left
.
opt_step 1940 | slm_loss (selected) 9.2721 | 57.8s for last 20 opt steps | ~954 opt steps left
.
opt_step 1960 | slm_loss (selected) 8.7199 | 40.9s for last 20 opt steps | ~934 opt steps left
.
opt_step 1980 | slm_loss (selected) 9.4472 | 49.9s for last 20 opt steps | ~914 opt steps left
.
opt_step 2000 | slm_loss (selected) 9.1717 | 33.8s for last 20 opt steps | ~894 opt steps left

opt_step 2020 | slm_loss (selected) 8.9851 | 30.8s for last 20 opt steps | ~874 opt steps left
.
opt_step 2040 | slm_loss (selected) 9.1157 | 30.8s for last 20 opt steps | ~854 opt steps left
.
opt_step 2060 | slm_loss (selected) 9.5022 | 30.8s for last 20 opt steps | ~834 opt steps left
.
opt_step 2080 | slm_loss (selected) 9.0212 | 37.0s for last 20 opt steps | ~814 opt steps left
.
opt_step 2100 | slm_loss (selected) 9.1591 | 31.0s for last 20 opt steps | ~794 opt steps left

opt_step 2120 | slm_loss (selected) 9.0266 | 30.7s for last 20 opt steps | ~774 opt steps left
.
opt_step 2140 | slm_loss (selected) 9.3868 | 30.2s for last 20 opt steps | ~754 opt steps left
.
opt_step 2160 | slm_loss (selected) 9.2802 | 30.0s for last 20 opt steps | ~734 opt steps left
.
opt_step 2180 | slm_loss (selected) 9.1717 | 29.5s for last 20 opt steps | ~714 opt steps left
.
opt_step 2200 | slm_loss (selected) 9.1373 | 29.3s for last 20 opt steps | ~694 opt steps left

opt_step 2220 | slm_loss (selected) 8.9209 | 29.4s for last 20 opt steps | ~674 opt steps left
.
opt_step 2240 | slm_loss (selected) 8.9764 | 29.4s for last 20 opt steps | ~654 opt steps left
.
opt_step 2260 | slm_loss (selected) 9.2398 | 29.4s for last 20 opt steps | ~634 opt steps left
.
opt_step 2280 | slm_loss (selected) 9.6932 | 29.5s for last 20 opt steps | ~614 opt steps left
.
opt_step 2300 | slm_loss (selected) 9.0584 | 29.7s for last 20 opt steps | ~594 opt steps left

opt_step 2320 | slm_loss (selected) 9.0779 | 30.0s for last 20 opt steps | ~574 opt steps left
.
opt_step 2340 | slm_loss (selected) 8.9558 | 29.8s for last 20 opt steps | ~554 opt steps left
.
opt_step 2360 | slm_loss (selected) 9.2266 | 29.9s for last 20 opt steps | ~534 opt steps left
.
opt_step 2380 | slm_loss (selected) 9.1659 | 30.1s for last 20 opt steps | ~514 opt steps left
.
opt_step 2400 | slm_loss (selected) 9.3129 | 30.1s for last 20 opt steps | ~494 opt steps left

opt_step 2420 | slm_loss (selected) 9.0355 | 30.1s for last 20 opt steps | ~474 opt steps left
.
opt_step 2440 | slm_loss (selected) 9.0822 | 30.1s for last 20 opt steps | ~454 opt steps left
.
opt_step 2460 | slm_loss (selected) 9.4035 | 30.2s for last 20 opt steps | ~434 opt steps left
.
opt_step 2480 | slm_loss (selected) 8.9897 | 30.0s for last 20 opt steps | ~414 opt steps left
.
opt_step 2500 | slm_loss (selected) 9.5025 | 30.3s for last 20 opt steps | ~394 opt steps left

opt_step 2520 | slm_loss (selected) 9.4256 | 30.3s for last 20 opt steps | ~374 opt steps left
.
opt_step 2540 | slm_loss (selected) 9.1211 | 30.2s for last 20 opt steps | ~354 opt steps left
.
opt_step 2560 | slm_loss (selected) 9.1189 | 30.2s for last 20 opt steps | ~334 opt steps left
.
opt_step 2580 | slm_loss (selected) 9.2802 | 30.3s for last 20 opt steps | ~314 opt steps left
.
opt_step 2600 | slm_loss (selected) 9.3283 | 30.4s for last 20 opt steps | ~294 opt steps left

opt_step 2620 | slm_loss (selected) 9.0065 | 30.4s for last 20 opt steps | ~274 opt steps left
.
opt_step 2640 | slm_loss (selected) 9.3640 | 30.3s for last 20 opt steps | ~254 opt steps left
.
opt_step 2660 | slm_loss (selected) 9.4454 | 30.3s for last 20 opt steps | ~234 opt steps left
.
opt_step 2680 | slm_loss (selected) 9.2746 | 30.3s for last 20 opt steps | ~214 opt steps left
.
opt_step 2700 | slm_loss (selected) 9.4121 | 30.3s for last 20 opt steps | ~194 opt steps left

opt_step 2720 | slm_loss (selected) 9.1375 | 30.3s for last 20 opt steps | ~174 opt steps left
.
opt_step 2740 | slm_loss (selected) 9.1473 | 30.6s for last 20 opt steps | ~154 opt steps left
.
opt_step 2760 | slm_loss (selected) 9.5536 | 30.2s for last 20 opt steps | ~134 opt steps left
.
opt_step 2780 | slm_loss (selected) 9.6189 | 30.5s for last 20 opt steps | ~114 opt steps left
.
opt_step 2800 | slm_loss (selected) 9.2763 | 30.6s for last 20 opt steps | ~94 opt steps left

opt_step 2820 | slm_loss (selected) 9.5868 | 30.7s for last 20 opt steps | ~74 opt steps left
.
opt_step 2840 | slm_loss (selected) 9.4065 | 30.6s for last 20 opt steps | ~54 opt steps left
.
opt_step 2860 | slm_loss (selected) 9.1721 | 30.6s for last 20 opt steps | ~34 opt steps left
.
opt_step 2880 | slm_loss (selected) 8.9970 | 30.8s for last 20 opt steps | ~14 opt steps left
End epoch 1: val_loss=2.4451 val_ppl=11.53
Saving student model (LoRA adapters) → slm_tinyllama_topk_r03_e1
Done.
