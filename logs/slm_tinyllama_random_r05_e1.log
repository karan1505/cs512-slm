Loading dataset…
Data ready. Steps per epoch: 11578 (batch=4, grad_accum=4)
Loading reference model…
`torch_dtype` is deprecated! Use `dtype` instead!
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading student base model…
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Attaching LoRA adapters from baseline_tinyllama_lora_bs4_ga4_e1 …
trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338
Starting SLM-LoRA training… selection=random, select_ratio=0.5
Epoch 1/1 …
/home/karans/miniconda3/envs/slm512/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/karans/miniconda3/envs/slm512/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")

opt_step 20 | slm_loss (selected) 8.8180 | 28.4s for last 20 opt steps | ~2874 opt steps left
.
opt_step 40 | slm_loss (selected) 8.9909 | 27.4s for last 20 opt steps | ~2854 opt steps left
.
opt_step 60 | slm_loss (selected) 8.8175 | 27.6s for last 20 opt steps | ~2834 opt steps left
.
opt_step 80 | slm_loss (selected) 8.9495 | 27.5s for last 20 opt steps | ~2814 opt steps left
.
opt_step 100 | slm_loss (selected) 8.7207 | 27.5s for last 20 opt steps | ~2794 opt steps left

opt_step 120 | slm_loss (selected) 8.7689 | 27.5s for last 20 opt steps | ~2774 opt steps left
.
opt_step 140 | slm_loss (selected) 8.8409 | 27.8s for last 20 opt steps | ~2754 opt steps left
.
opt_step 160 | slm_loss (selected) 8.9027 | 27.7s for last 20 opt steps | ~2734 opt steps left
.
opt_step 180 | slm_loss (selected) 8.9708 | 27.7s for last 20 opt steps | ~2714 opt steps left
.
opt_step 200 | slm_loss (selected) 8.8348 | 28.1s for last 20 opt steps | ~2694 opt steps left

opt_step 220 | slm_loss (selected) 8.7061 | 27.8s for last 20 opt steps | ~2674 opt steps left
.
opt_step 240 | slm_loss (selected) 8.7829 | 27.8s for last 20 opt steps | ~2654 opt steps left
.
opt_step 260 | slm_loss (selected) 8.5410 | 27.8s for last 20 opt steps | ~2634 opt steps left
.
opt_step 280 | slm_loss (selected) 8.7725 | 27.8s for last 20 opt steps | ~2614 opt steps left
.
opt_step 300 | slm_loss (selected) 8.6593 | 28.0s for last 20 opt steps | ~2594 opt steps left

opt_step 320 | slm_loss (selected) 8.6631 | 27.9s for last 20 opt steps | ~2574 opt steps left
.
opt_step 340 | slm_loss (selected) 8.7039 | 28.0s for last 20 opt steps | ~2554 opt steps left
.
opt_step 360 | slm_loss (selected) 8.7110 | 28.0s for last 20 opt steps | ~2534 opt steps left
.
opt_step 380 | slm_loss (selected) 8.5578 | 28.2s for last 20 opt steps | ~2514 opt steps left
.
opt_step 400 | slm_loss (selected) 8.6213 | 28.1s for last 20 opt steps | ~2494 opt steps left

opt_step 420 | slm_loss (selected) 8.5768 | 28.1s for last 20 opt steps | ~2474 opt steps left
.
opt_step 440 | slm_loss (selected) 8.5162 | 28.1s for last 20 opt steps | ~2454 opt steps left
.
opt_step 460 | slm_loss (selected) 8.8728 | 28.1s for last 20 opt steps | ~2434 opt steps left
.
opt_step 480 | slm_loss (selected) 8.9330 | 28.3s for last 20 opt steps | ~2414 opt steps left
.
opt_step 500 | slm_loss (selected) 8.6758 | 28.2s for last 20 opt steps | ~2394 opt steps left

opt_step 520 | slm_loss (selected) 8.8916 | 28.2s for last 20 opt steps | ~2374 opt steps left
.
opt_step 540 | slm_loss (selected) 8.7414 | 28.2s for last 20 opt steps | ~2354 opt steps left
.
opt_step 560 | slm_loss (selected) 8.7014 | 28.4s for last 20 opt steps | ~2334 opt steps left
.
opt_step 580 | slm_loss (selected) 8.7629 | 28.3s for last 20 opt steps | ~2314 opt steps left
.
opt_step 600 | slm_loss (selected) 8.7114 | 28.3s for last 20 opt steps | ~2294 opt steps left

opt_step 620 | slm_loss (selected) 8.8517 | 28.3s for last 20 opt steps | ~2274 opt steps left
.
opt_step 640 | slm_loss (selected) 8.6102 | 28.3s for last 20 opt steps | ~2254 opt steps left
.
opt_step 660 | slm_loss (selected) 8.5460 | 28.5s for last 20 opt steps | ~2234 opt steps left
.
opt_step 680 | slm_loss (selected) 8.6990 | 28.3s for last 20 opt steps | ~2214 opt steps left
.
opt_step 700 | slm_loss (selected) 8.7533 | 28.3s for last 20 opt steps | ~2194 opt steps left

opt_step 720 | slm_loss (selected) 8.8307 | 28.4s for last 20 opt steps | ~2174 opt steps left
.
opt_step 740 | slm_loss (selected) 8.8695 | 28.5s for last 20 opt steps | ~2154 opt steps left
.
opt_step 760 | slm_loss (selected) 8.8067 | 28.3s for last 20 opt steps | ~2134 opt steps left
.
opt_step 780 | slm_loss (selected) 8.6307 | 28.4s for last 20 opt steps | ~2114 opt steps left
.
opt_step 800 | slm_loss (selected) 8.4129 | 28.3s for last 20 opt steps | ~2094 opt steps left

opt_step 820 | slm_loss (selected) 8.9485 | 28.5s for last 20 opt steps | ~2074 opt steps left
.
opt_step 840 | slm_loss (selected) 8.7236 | 28.3s for last 20 opt steps | ~2054 opt steps left
.
opt_step 860 | slm_loss (selected) 8.5201 | 28.4s for last 20 opt steps | ~2034 opt steps left
.
opt_step 880 | slm_loss (selected) 8.6606 | 28.4s for last 20 opt steps | ~2014 opt steps left
.
opt_step 900 | slm_loss (selected) 8.8416 | 28.6s for last 20 opt steps | ~1994 opt steps left

opt_step 920 | slm_loss (selected) 8.6067 | 28.5s for last 20 opt steps | ~1974 opt steps left
.
opt_step 940 | slm_loss (selected) 9.0026 | 28.7s for last 20 opt steps | ~1954 opt steps left
.
opt_step 960 | slm_loss (selected) 8.7652 | 29.1s for last 20 opt steps | ~1934 opt steps left
.
opt_step 980 | slm_loss (selected) 8.7971 | 28.6s for last 20 opt steps | ~1914 opt steps left
.
opt_step 1000 | slm_loss (selected) 8.7663 | 28.4s for last 20 opt steps | ~1894 opt steps left

opt_step 1020 | slm_loss (selected) 8.6467 | 28.4s for last 20 opt steps | ~1874 opt steps left
.
opt_step 1040 | slm_loss (selected) 8.7643 | 28.5s for last 20 opt steps | ~1854 opt steps left
.
opt_step 1060 | slm_loss (selected) 8.6233 | 28.6s for last 20 opt steps | ~1834 opt steps left
.
opt_step 1080 | slm_loss (selected) 9.0096 | 28.4s for last 20 opt steps | ~1814 opt steps left
.
opt_step 1100 | slm_loss (selected) 8.7622 | 28.4s for last 20 opt steps | ~1794 opt steps left

opt_step 1120 | slm_loss (selected) 8.8542 | 28.4s for last 20 opt steps | ~1774 opt steps left
.
opt_step 1140 | slm_loss (selected) 8.9861 | 28.4s for last 20 opt steps | ~1754 opt steps left
.
opt_step 1160 | slm_loss (selected) 8.8567 | 28.5s for last 20 opt steps | ~1734 opt steps left
.
opt_step 1180 | slm_loss (selected) 8.5474 | 28.4s for last 20 opt steps | ~1714 opt steps left
.
opt_step 1200 | slm_loss (selected) 8.8935 | 28.4s for last 20 opt steps | ~1694 opt steps left

opt_step 1220 | slm_loss (selected) 8.8115 | 28.5s for last 20 opt steps | ~1674 opt steps left
.
opt_step 1240 | slm_loss (selected) 8.9006 | 28.4s for last 20 opt steps | ~1654 opt steps left
.
opt_step 1260 | slm_loss (selected) 8.7187 | 28.4s for last 20 opt steps | ~1634 opt steps left
.
opt_step 1280 | slm_loss (selected) 8.7870 | 28.4s for last 20 opt steps | ~1614 opt steps left
.
opt_step 1300 | slm_loss (selected) 8.4412 | 28.4s for last 20 opt steps | ~1594 opt steps left

opt_step 1320 | slm_loss (selected) 8.5901 | 28.5s for last 20 opt steps | ~1574 opt steps left
.
opt_step 1340 | slm_loss (selected) 8.8610 | 28.4s for last 20 opt steps | ~1554 opt steps left
.
opt_step 1360 | slm_loss (selected) 8.6554 | 28.5s for last 20 opt steps | ~1534 opt steps left
.
opt_step 1380 | slm_loss (selected) 8.6368 | 28.6s for last 20 opt steps | ~1514 opt steps left
.
opt_step 1400 | slm_loss (selected) 8.8246 | 28.4s for last 20 opt steps | ~1494 opt steps left

opt_step 1420 | slm_loss (selected) 8.8469 | 28.4s for last 20 opt steps | ~1474 opt steps left
.
opt_step 1440 | slm_loss (selected) 8.7085 | 28.4s for last 20 opt steps | ~1454 opt steps left
.
opt_step 1460 | slm_loss (selected) 8.7833 | 28.6s for last 20 opt steps | ~1434 opt steps left
.
opt_step 1480 | slm_loss (selected) 8.5531 | 28.4s for last 20 opt steps | ~1414 opt steps left
.
opt_step 1500 | slm_loss (selected) 8.7696 | 28.4s for last 20 opt steps | ~1394 opt steps left

opt_step 1520 | slm_loss (selected) 9.0042 | 28.5s for last 20 opt steps | ~1374 opt steps left
.
opt_step 1540 | slm_loss (selected) 8.8721 | 28.4s for last 20 opt steps | ~1354 opt steps left
.
opt_step 1560 | slm_loss (selected) 8.5546 | 28.4s for last 20 opt steps | ~1334 opt steps left
.
opt_step 1580 | slm_loss (selected) 8.8157 | 28.4s for last 20 opt steps | ~1314 opt steps left
.
opt_step 1600 | slm_loss (selected) 8.5363 | 28.6s for last 20 opt steps | ~1294 opt steps left

opt_step 1620 | slm_loss (selected) 8.4692 | 28.4s for last 20 opt steps | ~1274 opt steps left
.
opt_step 1640 | slm_loss (selected) 8.8923 | 28.4s for last 20 opt steps | ~1254 opt steps left
.
opt_step 1660 | slm_loss (selected) 8.8572 | 28.6s for last 20 opt steps | ~1234 opt steps left
.
opt_step 1680 | slm_loss (selected) 8.6710 | 28.4s for last 20 opt steps | ~1214 opt steps left
.
opt_step 1700 | slm_loss (selected) 8.6290 | 28.4s for last 20 opt steps | ~1194 opt steps left

opt_step 1720 | slm_loss (selected) 8.5157 | 28.5s for last 20 opt steps | ~1174 opt steps left
.
opt_step 1740 | slm_loss (selected) 8.6748 | 28.7s for last 20 opt steps | ~1154 opt steps left
.
opt_step 1760 | slm_loss (selected) 8.7363 | 28.5s for last 20 opt steps | ~1134 opt steps left
.
opt_step 1780 | slm_loss (selected) 8.7626 | 28.5s for last 20 opt steps | ~1114 opt steps left
.
opt_step 1800 | slm_loss (selected) 8.8435 | 28.7s for last 20 opt steps | ~1094 opt steps left

opt_step 1820 | slm_loss (selected) 8.6956 | 28.5s for last 20 opt steps | ~1074 opt steps left
.
opt_step 1840 | slm_loss (selected) 8.6876 | 28.5s for last 20 opt steps | ~1054 opt steps left
.
opt_step 1860 | slm_loss (selected) 8.7979 | 28.5s for last 20 opt steps | ~1034 opt steps left
.
opt_step 1880 | slm_loss (selected) 8.7585 | 28.7s for last 20 opt steps | ~1014 opt steps left
.
opt_step 1900 | slm_loss (selected) 8.8814 | 28.6s for last 20 opt steps | ~994 opt steps left

opt_step 1920 | slm_loss (selected) 8.9091 | 28.6s for last 20 opt steps | ~974 opt steps left
.
opt_step 1940 | slm_loss (selected) 8.8694 | 28.6s for last 20 opt steps | ~954 opt steps left
.
opt_step 1960 | slm_loss (selected) 8.6032 | 28.7s for last 20 opt steps | ~934 opt steps left
.
opt_step 1980 | slm_loss (selected) 8.6964 | 28.6s for last 20 opt steps | ~914 opt steps left
.
opt_step 2000 | slm_loss (selected) 8.9488 | 28.6s for last 20 opt steps | ~894 opt steps left

opt_step 2020 | slm_loss (selected) 8.7573 | 28.5s for last 20 opt steps | ~874 opt steps left
.
opt_step 2040 | slm_loss (selected) 8.7524 | 28.7s for last 20 opt steps | ~854 opt steps left
.
opt_step 2060 | slm_loss (selected) 8.5508 | 28.6s for last 20 opt steps | ~834 opt steps left
.
opt_step 2080 | slm_loss (selected) 8.7981 | 28.5s for last 20 opt steps | ~814 opt steps left
.
opt_step 2100 | slm_loss (selected) 9.0819 | 28.3s for last 20 opt steps | ~794 opt steps left

opt_step 2120 | slm_loss (selected) 8.5409 | 28.4s for last 20 opt steps | ~774 opt steps left
.
opt_step 2140 | slm_loss (selected) 8.8670 | 28.3s for last 20 opt steps | ~754 opt steps left
.
opt_step 2160 | slm_loss (selected) 8.6190 | 28.2s for last 20 opt steps | ~734 opt steps left
.
opt_step 2180 | slm_loss (selected) 8.6543 | 28.2s for last 20 opt steps | ~714 opt steps left
.
opt_step 2200 | slm_loss (selected) 8.7054 | 28.4s for last 20 opt steps | ~694 opt steps left

opt_step 2220 | slm_loss (selected) 8.9372 | 28.3s for last 20 opt steps | ~674 opt steps left
.
opt_step 2240 | slm_loss (selected) 8.6643 | 28.2s for last 20 opt steps | ~654 opt steps left
.
opt_step 2260 | slm_loss (selected) 8.4961 | 28.3s for last 20 opt steps | ~634 opt steps left
.
opt_step 2280 | slm_loss (selected) 8.5237 | 28.2s for last 20 opt steps | ~614 opt steps left
.
opt_step 2300 | slm_loss (selected) 8.6624 | 28.4s for last 20 opt steps | ~594 opt steps left

opt_step 2320 | slm_loss (selected) 8.7866 | 28.2s for last 20 opt steps | ~574 opt steps left
.
opt_step 2340 | slm_loss (selected) 8.9082 | 28.3s for last 20 opt steps | ~554 opt steps left
.
opt_step 2360 | slm_loss (selected) 8.7531 | 28.3s for last 20 opt steps | ~534 opt steps left
.
opt_step 2380 | slm_loss (selected) 8.6773 | 28.4s for last 20 opt steps | ~514 opt steps left
.
opt_step 2400 | slm_loss (selected) 8.8183 | 28.2s for last 20 opt steps | ~494 opt steps left

opt_step 2420 | slm_loss (selected) 8.6784 | 28.4s for last 20 opt steps | ~474 opt steps left
.
opt_step 2440 | slm_loss (selected) 8.6639 | 28.5s for last 20 opt steps | ~454 opt steps left
.
opt_step 2460 | slm_loss (selected) 8.5409 | 28.6s for last 20 opt steps | ~434 opt steps left
.
opt_step 2480 | slm_loss (selected) 8.7675 | 28.8s for last 20 opt steps | ~414 opt steps left
.
opt_step 2500 | slm_loss (selected) 8.6651 | 28.6s for last 20 opt steps | ~394 opt steps left

opt_step 2520 | slm_loss (selected) 8.8086 | 28.6s for last 20 opt steps | ~374 opt steps left
.
opt_step 2540 | slm_loss (selected) 8.7692 | 28.6s for last 20 opt steps | ~354 opt steps left
.
opt_step 2560 | slm_loss (selected) 8.7727 | 28.8s for last 20 opt steps | ~334 opt steps left
.
opt_step 2580 | slm_loss (selected) 8.7568 | 28.6s for last 20 opt steps | ~314 opt steps left
.
opt_step 2600 | slm_loss (selected) 8.6804 | 28.6s for last 20 opt steps | ~294 opt steps left

opt_step 2620 | slm_loss (selected) 8.8725 | 28.6s for last 20 opt steps | ~274 opt steps left
.
opt_step 2640 | slm_loss (selected) 8.7609 | 28.7s for last 20 opt steps | ~254 opt steps left
.
opt_step 2660 | slm_loss (selected) 8.5729 | 28.8s for last 20 opt steps | ~234 opt steps left
.
opt_step 2680 | slm_loss (selected) 9.0202 | 28.7s for last 20 opt steps | ~214 opt steps left
.
opt_step 2700 | slm_loss (selected) 8.5865 | 28.6s for last 20 opt steps | ~194 opt steps left

opt_step 2720 | slm_loss (selected) 8.8323 | 28.7s for last 20 opt steps | ~174 opt steps left
.
opt_step 2740 | slm_loss (selected) 8.7574 | 28.6s for last 20 opt steps | ~154 opt steps left
.
opt_step 2760 | slm_loss (selected) 8.8113 | 28.6s for last 20 opt steps | ~134 opt steps left
.
opt_step 2780 | slm_loss (selected) 8.9166 | 28.6s for last 20 opt steps | ~114 opt steps left
.
opt_step 2800 | slm_loss (selected) 8.8448 | 28.8s for last 20 opt steps | ~94 opt steps left

opt_step 2820 | slm_loss (selected) 8.7735 | 28.6s for last 20 opt steps | ~74 opt steps left
.
opt_step 2840 | slm_loss (selected) 8.4943 | 28.6s for last 20 opt steps | ~54 opt steps left
.
opt_step 2860 | slm_loss (selected) 8.7183 | 28.6s for last 20 opt steps | ~34 opt steps left
.
opt_step 2880 | slm_loss (selected) 8.8039 | 28.8s for last 20 opt steps | ~14 opt steps left
End epoch 1: val_loss=2.3074 val_ppl=10.05
Saving student model (LoRA adapters) → slm_tinyllama_random_r05_e1
Done.
