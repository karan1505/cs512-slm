Using device: cuda
Config: epochs=1, batch_size=8, select_ratio=0.5
Loading WikiText-2...
Train blocks: 18666 Val blocks: 1931
Loading reference model: gpt2
Loading student model: gpt2
Starting SLM training with select_ratio = 0.5
Epoch 1 Step 100/2334 - SLM loss (selected tokens only): 4.2270
Epoch 1 Step 200/2334 - SLM loss (selected tokens only): 4.0168
Epoch 1 Step 300/2334 - SLM loss (selected tokens only): 3.9576
Epoch 1 Step 400/2334 - SLM loss (selected tokens only): 3.9135
Epoch 1 Step 500/2334 - SLM loss (selected tokens only): 3.9023
Epoch 1 Step 600/2334 - SLM loss (selected tokens only): 3.8378
Epoch 1 Step 700/2334 - SLM loss (selected tokens only): 3.8381
Epoch 1 Step 800/2334 - SLM loss (selected tokens only): 3.8154
Epoch 1 Step 900/2334 - SLM loss (selected tokens only): 3.7671
Epoch 1 Step 1000/2334 - SLM loss (selected tokens only): 3.8121
Epoch 1 Step 1100/2334 - SLM loss (selected tokens only): 3.7700
Epoch 1 Step 1200/2334 - SLM loss (selected tokens only): 3.7870
Epoch 1 Step 1300/2334 - SLM loss (selected tokens only): 3.7742
Epoch 1 Step 1400/2334 - SLM loss (selected tokens only): 3.7615
Epoch 1 Step 1500/2334 - SLM loss (selected tokens only): 3.7522
Epoch 1 Step 1600/2334 - SLM loss (selected tokens only): 3.7338
Epoch 1 Step 1700/2334 - SLM loss (selected tokens only): 3.7196
Epoch 1 Step 1800/2334 - SLM loss (selected tokens only): 3.7179
Epoch 1 Step 1900/2334 - SLM loss (selected tokens only): 3.7146
Epoch 1 Step 2000/2334 - SLM loss (selected tokens only): 3.7117
Epoch 1 Step 2100/2334 - SLM loss (selected tokens only): 3.7232
Epoch 1 Step 2200/2334 - SLM loss (selected tokens only): 3.6970
Epoch 1 Step 2300/2334 - SLM loss (selected tokens only): 3.6730
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
End of epoch 1: val_loss (full tokens) = 3.6362, val_ppl = 37.95
SLM training complete.
