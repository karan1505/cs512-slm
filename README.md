# Selective Language Modeling (SLM) â€” TinyLlama-1.1B Replication

This project reproduces the core empirical results from the NeurIPS 2024 paper **â€œNot All Tokens Are What You Need for Pretrainingâ€** using **TinyLlama-1.1B** on a single consumer GPU.

We evaluate:

- **Baseline CLM training**
- **Top-k SLM**
- **Random SLM**
- **Stochastic SLM**

across selection ratios **r = 0.5** and **r = 0.3**, along with:

- Token bucket movement (Hâ†’L, Lâ†’H)
- Validation perplexity
- Cosine similarity between LoRA update directions

This produces a compact, faithful replication of the paperâ€™s key findings.

---

## ğŸ“Œ Baseline CLM Training (Full Tokens)

Serves as the reference model (100% token usage).

### Baseline Training Loss Curve
![Baseline Loss](clm_baseline_loss_curve.png)

**Final validation perplexity:** â‰ˆ **9.89**

---

# ğŸ” Results â€” Select Ratio **r = 0.5**

## Top-k SLM (r = 0.5)

![Top-k Loss](slm_topk_loss_curve.png)

**Observations:**
- Validation ppl â‰ˆ **11.40**
- Worst performer
- Highly unstable
- Confirms paper: **hard selection harms performance**

---

## Stochastic SLM (r = 0.5)

![Stochastic 0.5](slm_stochastic_loss_curve.png)

**Observations:**
- Validation ppl â‰ˆ **10.04**
- Closest to baseline
- Soft preference for high-loss tokens â†’ stable

---

## Random SLM (r = 0.5)

![Random 0.5](slm_random_loss_curve.png)

**Observations:**
- Validation ppl â‰ˆ **10.05**
- Nearly identical to stochastic
- Strong evidence of **token redundancy**

---

# ğŸ” Results â€” Select Ratio **r = 0.3**

## Stochastic SLM (r = 0.3)

![Stochastic 0.3](slm_stochastic_loss_r0.3curve.png)

**Observations:**
- More noise (fewer tokens)
- Still stable and effective
- Shows SLM holds up even at 30% token usage

---

## Top-k SLM (r = 0.3)

![Top-k r=0.3](slm_topk_r03_loss.png)

**Observations:**
- Noisy but structured
- Worse than stochastic at same ratio
- Still confirms difficulty-targeting behavior

---

# ğŸ“Š Token Bucket Transition Analysis (Hâ†’L / Lâ†’H)

Tokens are bucketed using the **baseline 70% loss quantile**.

### Interpretation:

- **Top-k:**
  - Highest **Lâ†’H** regressions (bad)
  - Confirms overfitting to difficult tokens

- **Random:**
  - Moves tokens without pattern
  - Neutral but stable

- **Stochastic:**
  - Highest **Hâ†’L** improvements
  - Lowest **Lâ†’H** regressions
  - Best stability  
  - Matches paperâ€™s motivation for **soft selection**

---

# ğŸ“ LoRA Update Cosine Similarity

Cosine similarity measures how close the update direction is to baseline CLM.

Higher = more similar to CLM training behavior.

![Cosine Similarity](cosine_similarity.png)

## Cosine Similarity vs Baseline LoRA

| Comparison | Cosine |
|-----------|--------|
| Baseline â†” Top-k | **0.9116** |
| Baseline â†” Random | **0.9317** |
| Baseline â†” Stochastic | **0.9304** |

â¡ **Random & Stochastic** remain closest to baseline.  
â¡ **Top-k** diverges the most â†’ explains poor validation loss.

---

## Cosine Similarity Between SLM Variants

| Comparison | Cosine |
|-----------|--------|
| Top-k â†” Random | 0.8763 |
| Top-k â†” Stochastic | 0.9020 |
| Random â†” Stochastic | **0.9109** |

Random â†” Stochastic is highest â†’ both behave as â€œsoft CLMâ€.

---

# ğŸ Final Replication Findings

## âœ” Token redundancy is real  
Using **50% of tokens** increases perplexity by only **~1.5%**.

## âœ” Stochastic SLM is the most stable  
Closest to baseline in:
- Perplexity  
- Token bucket behavior  
- Cosine similarity  

## âœ” Top-k selection is harmful  
Deterministic hard selection hurts quality and stability.

## âœ” Training direction preserved under Random/Stochastic  
LoRA cosine similarities confirm close alignment with CLM.

## âœ” This project successfully reproduces the NeurIPS 2024 paperâ€™s main claims  
All trends match the behaviors described in the SLM/RHO-1 paper.
